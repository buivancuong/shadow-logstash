input {
    file {
        file_completed_action => "log_and_delete"
        file_completed_log_path => "/var/log/logstash/completed.log"
        file_sort_by => "path"
        mode => "read"
        path => "/var/log/uploaded/Microsoft-Sinkhole*.csv"
        sincedb_clean_after => 0.125
        sincedb_path => "/dev/null"
    }
}
filter {
    if [message] =~ /^[,]+\r$/ { drop {} }
    csv {
        skip_empty_rows => true
        columns => [
            "LogTime", "SrcIP", "SrcIP_ASN", "geo", "URL", "Type", "http_agent", "tor",
            "SrcPort", "pOf_genre", "pOf_detail", "HostName", "DstPort", "MaliciousHostName",
            "referer", "referer_ip", "referer_asn", "referer_geo", "DstIP", "DstIP_ASN",
            "DstIPGeo", "naics", "sic", "referer_naics", "referer_sic", "Sector", "ssl_cipher",
            "application", "version", "Orgs"
        ]
    }
    if "LogTime" == "timestamp" { drop {} }
    grok { match => { "path" => "/var/log/uploaded/%{GREEDYDATA:FileName}" } }
    ruby { code => "event.to_hash.each { |k, v| event.set(k, '-') if v == nil }" }
    if [SrcIP] == "-" { mutate { replace => { "SrcIP" => "0.0.0.0" } } }
    if [DstIP] == "-" { mutate { replace => { "DstIP" => "0.0.0.0" } } }
    mutate { split => { "Orgs" => "," } }
    prune { whitelist_names => ["@timestamp","@version","message","tags","^[A-Z]"] }
}
output {
    elasticsearch {
        hosts => ["localhost:8686"]
        manage_template => false
        index => "drone_sinkhole"
        template => "/etc/logstash/template.d/drone_sinkhole.json"
        http_compression => true
        ssl => true
        ssl_certificate_verification => false
        user => "logstash_user"
        password => "Logst4sh@123"
    }
}
